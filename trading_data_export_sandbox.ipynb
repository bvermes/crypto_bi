{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/exports'\n",
    "\n",
    "timeframe_array = ['1m','15m','1h','4h', '1d', '1w']\n",
    "timeframe_mapping = {'1m': '1min', '15m': '15min', '1h': '1h', '4h': '4h', '1d': '1D', '1w': '7D'}\n",
    "\n",
    "timeframe_array = [\n",
    "                '1m','15m','1h','4h', '1d', '1w',\n",
    "                #'1M'\n",
    "                ]\n",
    "symbol_array = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT','ICPUSDT', 'AVAXUSDT']\n",
    "\n",
    "interval_to_ms = {\n",
    "    \"1m\": 60_000,\n",
    "    \"3m\": 3 * 60_000,\n",
    "    \"5m\": 5 * 60_000,\n",
    "    \"15m\": 15 * 60_000,\n",
    "    \"30m\": 30 * 60_000,\n",
    "    \"1h\": 60 * 60_000,\n",
    "    \"2h\": 2 * 60 * 60_000,\n",
    "    \"4h\": 4 * 60 * 60_000,\n",
    "    \"6h\": 6 * 60 * 60_000,\n",
    "    \"8h\": 8 * 60 * 60_000,\n",
    "    \"12h\": 12 * 60 * 60_000,\n",
    "    \"1d\": 24 * 60 * 60_000,\n",
    "    \"3d\": 3 * 24 * 60 * 60_000,\n",
    "    \"1w\": 7 * 24 * 60 * 60_000,\n",
    "    \"1M\": 30 * 24 * 60 * 60_000  # approximate month (30 days)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import data.datasource.binance_api as ba \n",
    "\n",
    "import business.utils.trading_signals as ts\n",
    "import business.utils.trading_indicators as ti\n",
    "import presentation.plotter as pl\n",
    "\n",
    "\n",
    "base_start_time_1m = int(pd.Timestamp(\"2019-01-01\").timestamp() * 1000)\n",
    "base_start_time_other = int(pd.Timestamp(\"2015-01-01\").timestamp() * 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_data_path(symbol, interval):\n",
    "    \"\"\"Return the exact file path if the CSV exists (case-sensitive for intervals).\"\"\"\n",
    "    filename = f\"data/exports/{symbol}_{interval}_data.csv\"\n",
    "    return filename if os.path.exists(filename) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_timestamp_from_csv(filepath):\n",
    "    \"\"\"Retrieve the last timestamp from an existing CSV file.\"\"\"\n",
    "    df = pd.read_csv(filepath, usecols=[\"timestamp\"])\n",
    "    if df.empty:\n",
    "        return None\n",
    "    last_timestamp = df[\"timestamp\"].iloc[-1]\n",
    "    return int(pd.Timestamp(last_timestamp).timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_historical_data(symbol, interval, start_time):\n",
    "    if interval not in interval_to_ms:\n",
    "        raise ValueError(f\"Interval {interval} not supported.\")\n",
    "\n",
    "    delta = interval_to_ms[interval]\n",
    "\n",
    "    all_data = []\n",
    "    start_overall = time.time()\n",
    "    iteration_count = 0\n",
    "\n",
    "    while True:\n",
    "        iteration_count += 1\n",
    "        klines = ba.get_binance_klines(symbol, interval, start_time, limit=1000)\n",
    "        if not klines:\n",
    "            print(\"No more data returned from Binance.\")\n",
    "            break\n",
    "\n",
    "        all_data += klines\n",
    "        # Advance start_time by the appropriate millisecond delta for the interval.\n",
    "        start_time = klines[-1][0] + delta\n",
    "\n",
    "        elapsed = time.time() - start_overall\n",
    "        latest_ts = klines[-1][0]\n",
    "        latest_dt = pd.to_datetime(latest_ts, unit='ms')\n",
    "        print(f\"Iteration {iteration_count}: Latest timestamp: {latest_ts} ({latest_dt}), \"\n",
    "              f\"Total records: {len(all_data)}, Elapsed time: {elapsed:.2f} sec\")\n",
    "\n",
    "        # If fewer than 'limit' records are returned, assume we've reached the end.\n",
    "        if len(klines) < 1000:\n",
    "            print(f\"Iteration {iteration_count}: Last batch retrieved with {len(klines)} records. Ending extraction.\")\n",
    "            break\n",
    "\n",
    "        # Respect Binance rate limits.\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Convert collected data into a DataFrame.\n",
    "    df = pd.DataFrame(all_data, columns=[\n",
    "        \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "    ])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit='ms')\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    print(\"Data extraction complete. Sample data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Export to CSV.\n",
    "    df.to_csv(f'data/exports/{symbol}_{interval}_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_new_data(symbol, interval, start_time, csv_path):\n",
    "    \"\"\"Append new data to an existing CSV file and save.\"\"\"\n",
    "    # Fetch new data from Binance\n",
    "    new_data = []\n",
    "    iteration_count = 0\n",
    "    start_overall = time.time()\n",
    "\n",
    "    while True:\n",
    "        iteration_count += 1\n",
    "        klines = ba.get_binance_klines(symbol, interval, start_time, limit=1000)\n",
    "        if not klines:\n",
    "            print(f\"{symbol} {interval}: No more data from Binance.\")\n",
    "            break\n",
    "\n",
    "        new_data += klines\n",
    "        start_time = klines[-1][0] + interval_to_ms[interval]\n",
    "\n",
    "        elapsed = time.time() - start_overall\n",
    "        latest_ts = klines[-1][0]\n",
    "        latest_dt = pd.to_datetime(latest_ts, unit='ms')\n",
    "        print(f\"{symbol} {interval} | Iteration {iteration_count}: Latest timestamp: {latest_dt}, \"\n",
    "              f\"Total new records: {len(new_data)}, Elapsed: {elapsed:.2f}s\")\n",
    "\n",
    "        if len(klines) < 1000:\n",
    "            print(f\"{symbol} {interval}: Last batch retrieved with {len(klines)} records. Reached end.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(0.5)  # Respect Binance rate limits\n",
    "\n",
    "    if not new_data:\n",
    "        print(f\"{symbol} {interval}: No new data to append.\")\n",
    "        return\n",
    "\n",
    "    # Convert collected data into a DataFrame\n",
    "    df_new = pd.DataFrame(new_data, columns=[\n",
    "        \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "    ])\n",
    "    df_new[\"timestamp\"] = pd.to_datetime(df_new[\"timestamp\"], unit='ms')\n",
    "\n",
    "    # Load existing data\n",
    "    df_existing = pd.read_csv(csv_path)\n",
    "    df_existing[\"timestamp\"] = pd.to_datetime(df_existing[\"timestamp\"])\n",
    "\n",
    "    # Combine and remove duplicates\n",
    "    df_combined = pd.concat([df_existing, df_new]).drop_duplicates(subset=[\"timestamp\"]).sort_values(by=\"timestamp\")\n",
    "\n",
    "    # Save updated CSV\n",
    "    df_combined.to_csv(csv_path, index=False)\n",
    "    print(f\"{symbol} {interval}: Data appended and saved. Total records: {len(df_combined)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_or_update(symbol, interval):\n",
    "    \"\"\"Check if export exists, update if yes, otherwise download from scratch.\"\"\"\n",
    "    csv_path = get_existing_data_path(symbol, interval)\n",
    "\n",
    "    if csv_path:\n",
    "        print(f\"✅ {symbol} {interval}: Existing CSV found. Checking for updates...\")\n",
    "        last_timestamp = get_last_timestamp_from_csv(csv_path)\n",
    "        if last_timestamp:\n",
    "            print(f\"📅 Last recorded timestamp: {pd.to_datetime(last_timestamp, unit='ms')}\")\n",
    "            append_new_data(symbol, interval, last_timestamp, csv_path)\n",
    "        else:\n",
    "            start_time = base_start_time_1m if interval == \"1m\" else base_start_time_other\n",
    "            print(f\"⚠️ {symbol} {interval}: CSV is empty, starting from {pd.to_datetime(start_time, unit='ms')}...\")\n",
    "            export_historical_data(symbol, interval, start_time)\n",
    "    else:\n",
    "        start_time = base_start_time_1m if interval == \"1m\" else base_start_time_other\n",
    "        print(f\"🚀 {symbol} {interval}: No existing data. Starting from {pd.to_datetime(start_time, unit='ms')}...\")\n",
    "        export_historical_data(symbol, interval, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_timeframes():\n",
    "    data = {}\n",
    "    \n",
    "    # Load datasets into a dictionary\n",
    "    for symbol in [\n",
    "                'BTCUSDT',\n",
    "                # 'ETHUSDT',\n",
    "                #'SOLUSDT',\n",
    "                #'ICPUSDT',\n",
    "                #'AVAXUSDT'\n",
    "            ]:\n",
    "        for timeframe in timeframe_array:\n",
    "            file_path = os.path.join(data_folder, f'{symbol}_{timeframe}_data.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
    "                data[(symbol, timeframe)] = df.set_index('timestamp')\n",
    "    \n",
    "    for (symbol, timeframe), df in data.items():\n",
    "        higher_timeframes = [tf for tf in timeframe_array \n",
    "                             if pd.Timedelta(timeframe_mapping[tf]) > pd.Timedelta(timeframe_mapping[timeframe])]\n",
    "        \n",
    "        for ht in higher_timeframes:\n",
    "            if (symbol, ht) in data:\n",
    "                ht_df = data[(symbol, ht)].reindex(df.index, method='ffill')\n",
    "                df[f'{ht}_open'] = ht_df['open']\n",
    "                df[f'{ht}_high'] = ht_df['high']\n",
    "                df[f'{ht}_low'] = ht_df['low']\n",
    "                # Use the current close from the lower timeframe (since we don't know the higher timeframe's final close)\n",
    "                df[f'{ht}_close'] = df['close']\n",
    "        \n",
    "        output_path = os.path.join(data_folder, f'{symbol}_{timeframe}_data.csv')\n",
    "        df.reset_index().to_csv(output_path, index=False)\n",
    "        print(f'Saved merged file: {output_path}')\n",
    "def check_missing_intervals():\n",
    "    for timeframe, freq in timeframe_mapping.items():\n",
    "        for symbol in symbol_array:\n",
    "            file_path = os.path.join(data_folder, f'{symbol}_{timeframe}_data.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
    "                df = df.set_index('timestamp')\n",
    "                \n",
    "                # Check for duplicates\n",
    "                duplicates = df.index.duplicated().sum()\n",
    "                if duplicates > 0:\n",
    "                    print(f'{symbol}_{timeframe}: {duplicates} duplicate timestamps found.')\n",
    "                \n",
    "                # Check for missing intervals\n",
    "                all_times = pd.date_range(start=df.index.min(), end=df.index.max(), freq=freq)\n",
    "                missing_times = all_times.difference(df.index)\n",
    "                if not missing_times.empty:\n",
    "                    print(f'{symbol}_{timeframe}: {len(missing_times)} missing timestamps.')\n",
    "                else:\n",
    "                    print(f'{symbol}_{timeframe}: No missing timestamps.')\n",
    "def find_missing_timestamps(df_name):\n",
    "    # Extract symbol and timeframe from dataframe name (e.g., 'BTCUSDT_1m')\n",
    "    symbol_timeframe = df_name.replace('_data', '').strip()  # Clean suffix if any\n",
    "    symbol, timeframe = symbol_timeframe.split('_')\n",
    "    \n",
    "    \n",
    "    file_path = f'data/exports/{df_name}.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(file_path, parse_dates=['timestamp']).set_index('timestamp')\n",
    "\n",
    "    if timeframe not in timeframe_mapping:\n",
    "        print(f\"Timeframe {timeframe} not recognized.\")\n",
    "        return None\n",
    "\n",
    "    # Create a full range of timestamps for the timeframe\n",
    "    freq = timeframe_mapping[timeframe]\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq=freq)\n",
    "\n",
    "    # Find missing timestamps\n",
    "    missing_timestamps = full_range.difference(df.index)\n",
    "\n",
    "    # Create a dataframe for the missing timestamps\n",
    "    missing_df = pd.DataFrame({'timestamp': missing_timestamps})\n",
    "    \n",
    "    if missing_df.empty:\n",
    "        print(f\"No missing timestamps for {df_name}.\")\n",
    "    else:\n",
    "        print(f\"Found {len(missing_df)} missing timestamps for {df_name}.\")\n",
    "\n",
    "    return missing_df\n",
    "\n",
    "def insert_indicator_values(df):\n",
    "    df['RSI'], df['RSI_MA'] = ti.calculate_rsi_with_ma(df['close'], rsi_period=14, ma_type=\"SMA\", ma_length=14)\n",
    "    df['MACD'], df['Signal'], df['MACD_Hist'] = ti.calculate_macd(df['close'], fast_period=12, slow_period=26, signal_period=9)\n",
    "    df['BB_Mid'], df['BB_Upper'], df['BB_Lower'] = ti.calculate_bollinger_bands(df['close'], window=20, num_std=2)\n",
    "    df['Stoch_K'], df['Stoch_D'] = ti.calculate_stochastic(df, k_period=14, d_period=3)\n",
    "    df = ti.calculate_fibonacci_from_swings(df = df, suffix='_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in symbol_array:\n",
    "        for interval in timeframe_array:\n",
    "            print(f\"📊 Processing {symbol} {interval}\")\n",
    "            export_or_update(symbol, interval)\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert higher timeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_timeframes()\n",
    "check_missing_intervals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = find_missing_timestamps('BTCUSDT_1m_data')\n",
    "if missing_df is not None and not missing_df.empty:\n",
    "    missing_df.to_csv('BTCUSDT_1m_missing_timestamps.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/exports/BTCUSDT_1m_data.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing = pd.read_csv('data/exports/BTCUSDT_1d_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_fib = ti.calculate_fibonacci_from_swings(df = df_existing, suffix='_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_fib.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot_fibonacci_chart(df_with_fib, selected_index=df_with_fib.index[-1], title=\"Fibonacci Retracement - 1D\", suffix=\"_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXPORT HISTORY\n",
    "\n",
    "symbol = \"BTCUSDT\"\n",
    "interval = \"1m\"\n",
    "start_time = int(pd.Timestamp(\"2019-01-01\").timestamp() * 1000)\n",
    "export_historical_data(symbol, interval, start_time)\n",
    "\n",
    "symbol = \"ETHUSDT\"\n",
    "interval = \"1m\"\n",
    "start_time = int(pd.Timestamp(\"2019-01-01\").timestamp() * 1000)\n",
    "export_historical_data(symbol, interval, start_time)\n",
    "\n",
    "symbol = \"SOLUSDT\"\n",
    "interval = \"1m\"\n",
    "start_time = int(pd.Timestamp(\"2019-01-01\").timestamp() * 1000)\n",
    "export_historical_data(symbol, interval, start_time)\n",
    "\n",
    "symbol = \"ICPUSDT\"\n",
    "interval = \"1m\"\n",
    "start_time = int(pd.Timestamp(\"2019-01-01\").timestamp() * 1000)\n",
    "export_historical_data(symbol, interval, start_time)\n",
    "\n",
    "symbol = \"AVAXUSDT\"\n",
    "interval = \"1m\"\n",
    "start_time = int(pd.Timestamp(\"2019-01-01\").timestamp() * 1000)\n",
    "export_historical_data(symbol, interval, start_time)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
